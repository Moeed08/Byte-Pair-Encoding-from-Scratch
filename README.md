# Overview
This repository contains an efficient implementation of Byte Pair Encoding (BPE) for subword tokenization. BPE is a data compression and tokenization algorithm that iteratively merges the most frequent adjacent character pairs to create meaningful subword units. This approach improves text representation and is widely used in natural language processing (NLP) tasks such as machine translation, language modeling, and text generation.

# Features
- Trainable BPE Model – Learn subword representations from raw text.
- Efficient Tokenization – Encode words into subword units for better text handling.
- Custom Vocabulary Size – Adjust vocabulary size based on dataset requirements.
- Encoding & Decoding – Convert words to subword tokens and reconstruct original words.
- Performance Evaluation – Analyze tokenization effectiveness on unseen text.

# Usage
 - Train the BPE model on a text corpus.
 - Encode words into subword tokens.
 - Decode tokens back into words.
 - valuate performance on unseen data.
